{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import random\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.signal import welch, resample, medfilt\n",
    "from scipy.interpolate import splev, splrep, interp1d\n",
    "print(np.__version__)\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name, base_dir = './'):\n",
    "    with open(os.path.join(base_dir, file_name), 'rb') as f: # read preprocessing result\n",
    "        ecg_data = joblib.load(f)\n",
    "    X, y, groups = ecg_data[\"X\"], ecg_data[\"y\"], ecg_data[\"groups\"]\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    del(ecg_data)\n",
    "    return X, y, groups\n",
    "\n",
    "def load_data(base_dir = './paper/dataset', file_name = \"apnea-ecg-physio-edr-train.pkl\"):\n",
    "    ir = 3 # interpolate interval\n",
    "    before = 2\n",
    "    after = 2\n",
    "    tm = np.arange(0, (before + 1 + after) * 60, step=1 / float(ir))\n",
    "\n",
    "    with open(os.path.join(base_dir, file_name), 'rb') as f: # read preprocessing result\n",
    "        apnea_ecg_train = joblib.load(f)\n",
    "    \n",
    "    print('\\n Processing training data...')\n",
    "    t = time.time()\n",
    "    x_train = []\n",
    "    o_train, edr_train, y_train = apnea_ecg_train[\"o_train\"], apnea_ecg_train[\"edr_train\"], apnea_ecg_train[\"y_train\"]\n",
    "    groups_train = apnea_ecg_train[\"groups_train\"]\n",
    "    for i in range(len(o_train)):\n",
    "        (rri_tm, rri_signal), (ampl_tm, ampl_siganl) = o_train[i]\n",
    "\t\t# Curve interpolation\n",
    "        if edr_train[i][0] == []:\n",
    "            print(\"Empty EDR signal!\")\n",
    "            continue\n",
    "        rri_interp_signal = splev(tm, splrep(rri_tm, rri_signal, k=3), ext=0) \n",
    "        ampl_interp_signal = splev(tm, splrep(ampl_tm, ampl_siganl, k=3), ext=0)\n",
    "        edr_interp_signal = resample(edr_train[i][0], 900)\n",
    "        x_train.append([rri_interp_signal, ampl_interp_signal, edr_interp_signal])\n",
    "    x_train = np.array(x_train, dtype=\"float32\").transpose((0, 2, 1)) # convert to numpy format\n",
    "    y_train = np.array(y_train, dtype=\"float32\")\n",
    "    print('\\n Done in ' + str(time.time() - t) + 's!')\n",
    "    del(apnea_ecg_train)\n",
    "\n",
    "    with open(os.path.join(base_dir, \"apnea-ecg-physio-edr-test.pkl\"), 'rb') as f: # read preprocessing result\n",
    "        apnea_ecg_test = joblib.load(f) \n",
    "\n",
    "    print('\\n Processing testing data...')\n",
    "    t = time.time()\n",
    "    x_test = []\n",
    "    o_test, edr_test, y_test = apnea_ecg_test[\"o_test\"], apnea_ecg_test[\"edr_test\"], apnea_ecg_test[\"y_test\"]\n",
    "    groups_test = apnea_ecg_test[\"groups_test\"]\n",
    "    for i in range(len(o_test)):\n",
    "        (rri_tm, rri_signal), (ampl_tm, ampl_siganl) = o_test[i]\n",
    "\t\t# Curve interpolation\n",
    "        if edr_test[i][0] == []:\n",
    "            print(\"Empty EDR signal!\")\n",
    "            continue\n",
    "        rri_interp_signal = splev(tm, splrep(rri_tm, rri_signal, k=3), ext=0)\n",
    "        ampl_interp_signal = splev(tm, splrep(ampl_tm, ampl_siganl, k=3), ext=0)\n",
    "        edr_interp_signal = resample(edr_test[i][0], 900)\n",
    "        x_test.append([rri_interp_signal, ampl_interp_signal, edr_interp_signal])\n",
    "    x_test = np.array(x_test, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_test = np.array(y_test, dtype=\"float32\")\n",
    "    print('\\n Done in ' + str(time.time() - t) + 's!')\n",
    "    del(apnea_ecg_test)\n",
    "\n",
    "    return x_train, y_train, groups_train, x_test, y_test, groups_test\n",
    "\n",
    "def load_data_cic(file_name, base_dir = './'):\n",
    "    with open(os.path.join(base_dir, file_name), 'rb') as f: # read preprocessing result\n",
    "        data = joblib.load(f)\n",
    "    X, sleep_stages, sleep_apnea, groups = data[\"X\"], data[\"sleep_stages\"], data['sleep_apnea'], data[\"groups\"]\n",
    "    X = np.array(X)\n",
    "    sleep_stages = np.array(sleep_stages)\n",
    "    sleep_apnea = np.array(sleep_apnea)\n",
    "    del(data)\n",
    "    return X, sleep_stages, sleep_apnea, groups\n",
    "\n",
    "def load_data_cic_new(file_name, base_dir = './'):\n",
    "    with open(os.path.join(base_dir, file_name), 'rb') as f: # read preprocessing result\n",
    "        data = joblib.load(f)\n",
    "    X, sleep_stages, sleep_apnea, sleep_apnea_locs, groups = data[\"X\"], data[\"sleep_stages\"], data['sleep_apnea'], data[\"sleep_apnea_locs\"], data[\"groups\"]\n",
    "    X = np.array(X)\n",
    "    sleep_stages = np.array(sleep_stages)\n",
    "    sleep_apnea = np.array(sleep_apnea)\n",
    "    del(data)\n",
    "    return X, sleep_stages, sleep_apnea, sleep_apnea_locs, groups\n",
    "\n",
    "def create_CNN(input_shape, normalize=False, n_class=6, \n",
    "                    CNN_n_layers=2, CNN_kernel_size=[5,5], CNN_n_nodes=[32,64], pooling_size=[3,3], cnn_strides=[2,2],\n",
    "                    dil_CNN_n_layers=2, dil_CNN_kernel_size=[5,5], dil_CNN_n_nodes=[32,64], dil_pooling_size=[3,3], dil_cnn_strides=[1,1], \n",
    "                    CNN_n_feature=32, drop_rate=0.2, weight=1e-3):\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    if normalize == True:\n",
    "        x = layers.BatchNormalization()(inputs)\n",
    "    if normalize == False:\n",
    "        x = inputs\n",
    "\n",
    "\t# Conv\n",
    "    for i in range(CNN_n_layers):\n",
    "        conv_layer = layers.Conv2D(CNN_n_nodes[i], kernel_size=(CNN_kernel_size[i],1), strides=(cnn_strides[i],1), padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=tf.keras.regularizers.l2(weight), bias_regularizer=tf.keras.regularizers.l2(weight), name=str('normal_conv_'+str(i)))\n",
    "        x = conv_layer(x)\n",
    "        max_pooling = layers.MaxPooling2D(pool_size=(pooling_size[i],1))\n",
    "        x = max_pooling(x)\n",
    "\n",
    "\n",
    "    #Dil conv\n",
    "    for i in range(dil_CNN_n_layers):\n",
    "        shape = x.shape.as_list()\n",
    "        print(shape)\n",
    "        dil_rate = int((shape[1]/2 - 3)/ 2) + 2\n",
    "        print('dilation rate = ', dil_rate)\n",
    "        conv_layer = layers.Conv2D(dil_CNN_n_nodes[i], kernel_size=(dil_CNN_kernel_size[i],1), strides=(dil_cnn_strides[i],1), dilation_rate=(dil_rate, 1), \n",
    "                padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(weight), bias_regularizer=tf.keras.regularizers.l2(weight), name=str('dilation_conv_'+str(i)))\n",
    "        x = conv_layer(x)\n",
    "        x = layers.Dropout(drop_rate)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(CNN_n_feature*4, activation=\"relu\")(x)\n",
    "    x = layers.Dense(CNN_n_feature, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(n_class, activation=\"softmax\")(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    return(model)\n",
    "\n",
    "def convert_class(y, n_class_ori=6, n_class=6):\n",
    "    if n_class_ori==6:\n",
    "        if n_class==6:\n",
    "            return(y)\n",
    "        y_conv = np.copy(y)\n",
    "        if n_class==5: # merge n3 and n4\n",
    "            y_conv[np.where(y==4)[0]] = 3\n",
    "            y_conv[np.where(y==5)[0]] = 4\n",
    "            return(y_conv)\n",
    "        if n_class==4: #merge n3 and n4, n1 and n2\n",
    "            y_conv[np.where(y==2)[0]] = 1\n",
    "            y_conv[np.where(y==3)[0]] = 2\n",
    "            y_conv[np.where(y==4)[0]] = 2\n",
    "            y_conv[np.where(y==5)[0]] = 3\n",
    "            return(y_conv)\n",
    "        if n_class==3:\n",
    "            y_conv[np.where(y==2)[0]] = 1\n",
    "            y_conv[np.where(y==3)[0]] = 1\n",
    "            y_conv[np.where(y==4)[0]] = 1\n",
    "            y_conv[np.where(y==5)[0]] = 2\n",
    "            return(y_conv)\n",
    "    if n_class_ori==5:\n",
    "        if n_class==5:\n",
    "            return(y)\n",
    "        y_conv = np.copy(y)\n",
    "        if n_class==4: #merge n3 and n4\n",
    "            y_conv[np.where(y==3)[0]] = 2\n",
    "            y_conv[np.where(y==4)[0]] = 3\n",
    "            return(y_conv)\n",
    "        if n_class==3: #merge n1 and n2, n3 and n4\n",
    "            y_conv[np.where(y==2)[0]] = 1\n",
    "            y_conv[np.where(y==3)[0]] = 2\n",
    "            y_conv[np.where(y==4)[0]] = 2\n",
    "            return(y_conv)\n",
    "\n",
    "\n",
    "def balance_class(X, y):\n",
    "    new_X = np.copy(X)\n",
    "    new_y = np.copy(y)\n",
    "    n_class = np.max(y) + 1\n",
    "    repartition = list(np.sum(np.array(to_categorical(y)), axis=0))\n",
    "    less_rpz_class = repartition.index(min(repartition))\n",
    "    for i in np.arange(n_class):\n",
    "        if i == less_rpz_class:\n",
    "            continue\n",
    "        too_much = repartition[i] - repartition[less_rpz_class] \n",
    "        idx = np.where(new_y==i)[0]\n",
    "        idx_to_remove = random.sample(range(0, len(idx)), int(too_much))\n",
    "        new_X = np.delete(new_X, idx[idx_to_remove], axis = 0)\n",
    "        new_y = np.delete(new_y, idx[idx_to_remove], axis = 0)\n",
    "    return(new_X, new_y)\n",
    "\n",
    "def convert_to_LSTM(X, y, groups, n_class):\n",
    "    y_conv = np.copy(y)\n",
    "    if n_class==5: # merge n3 and n4\n",
    "            y_conv[np.where(y==4)[0]] = 3\n",
    "            y_conv[np.where(y==5)[0]] = 4\n",
    "            y = y_conv\n",
    "    if n_class==4: #merge n3 and n4, n1 and n2\n",
    "            y_conv[np.where(y==2)[0]] = 1\n",
    "            y_conv[np.where(y==3)[0]] = 2\n",
    "            y_conv[np.where(y==4)[0]] = 2\n",
    "            y_conv[np.where(y==5)[0]] = 3\n",
    "            y = y_conv\n",
    "\n",
    "    y = np.array(to_categorical(y))\n",
    "\n",
    "    patient_id = [groups[0][0]]\n",
    "    for i in range(len(groups)):\n",
    "            if groups[i][0] == patient_id[-1]:\n",
    "                    continue\n",
    "            patient_id.append(groups[i][0])\n",
    "\n",
    "    ###########################\n",
    "    # Remove patient 4 because bad\n",
    "    ###########################\n",
    "\n",
    "    X_lstm = []\n",
    "    y_lstm = []\n",
    "    for j in range(len(patient_id)):\n",
    "            X_lstm_loc = []\n",
    "            y_lstm_loc = []\n",
    "            for i in range(X.shape[0]):\n",
    "                    if groups[i][0] == patient_id[j]:\n",
    "                            if X_lstm_loc == []:\n",
    "                                    X_lstm_loc = np.expand_dims(X[i], 0)\n",
    "                                    y_lstm_loc = np.expand_dims(y[i], 0)\n",
    "                            else:\n",
    "                                    X_lstm_loc = np.concatenate((X_lstm_loc, np.expand_dims(X[i], 0)))\n",
    "                                    y_lstm_loc = np.concatenate((y_lstm_loc, np.expand_dims(y[i], 0)))\n",
    "            X_lstm.append(np.expand_dims(X_lstm_loc, -1))\n",
    "            y_lstm.append(y_lstm_loc)\n",
    "\n",
    "    max_length = 0\n",
    "    for i in range(len(X_lstm)):\n",
    "        if X_lstm[i].shape[0] > max_length:\n",
    "            max_length = X_lstm[i].shape[0]\n",
    "    \n",
    "    for i in range(len(X_lstm)):\n",
    "        padding = int(max_length - X_lstm[i].shape[0])\n",
    "        conc = np.concatenate((np.zeros((max_length - padding, 1)), np.ones((padding, 1))))\n",
    "        X_lstm[i] = np.concatenate((X_lstm[i], np.zeros((padding, X_lstm[i].shape[1], 3, 1))))\n",
    "        y_lstm[i] = np.concatenate((y_lstm[i], np.zeros((padding, n_class))))\n",
    "        y_lstm[i] = np.concatenate((y_lstm[i], conc), axis = -1)\n",
    "    return(np.array(X_lstm), np.array(y_lstm))\n",
    "            \n",
    "\n",
    "def conf_matrix_CNN(X_test, y_test, model):\n",
    "        Y_test = y_test\n",
    "        y_pred = model.predict(X_test)\n",
    "        Y_pred = y_pred\n",
    "        y_test_matrix = np.zeros(len(Y_test))\n",
    "        y_pred_matrix = np.zeros(len(Y_pred))\n",
    "        for i in range(len(y_test_matrix)):\n",
    "                y_test_matrix[i] = np.argmax(Y_test[i])\n",
    "                y_pred_matrix[i] = np.argmax(Y_pred[i])\n",
    "        matrix = confusion_matrix(y_test_matrix, y_pred_matrix, normalize='pred')\n",
    "        sns.heatmap(matrix, annot=True, fmt='.2%', cmap='Blues')\n",
    "        return(matrix)\n",
    "\n",
    "def conf_matrix_LSTM(X_test, y_test, model):\n",
    "        Y_test = y_test.reshape(y_test.shape[0]*y_test.shape[1], y_test.shape[2])\n",
    "        y_pred = model.predict(X_test)\n",
    "        Y_pred = y_pred.reshape(y_pred.shape[0]*y_pred.shape[1], y_pred.shape[2])\n",
    "        y_test_matrix = np.zeros(len(Y_test))\n",
    "        y_pred_matrix = np.zeros(len(Y_pred))\n",
    "        for i in range(len(y_test_matrix)):\n",
    "                y_test_matrix[i] = np.argmax(Y_test[i])\n",
    "                y_pred_matrix[i] = np.argmax(Y_pred[i])\n",
    "        matrix = confusion_matrix(y_test_matrix, y_pred_matrix, normalize='pred')\n",
    "        sns.heatmap(matrix, annot=True, fmt='.2%', cmap='Blues')\n",
    "        plt.show()\n",
    "        return(matrix)\n",
    "\n",
    "def convert_to_CNN(y, n_class_ori, n_class):\n",
    "    y_cnn = convert_class(y, n_class_ori, n_class)\n",
    "    y_cnn = np.array(to_categorical(y_cnn))\n",
    "    return(y_cnn)\n",
    "\n",
    "def get_psd(x):\n",
    "    fs, psd = welch(x, fs=3)\n",
    "    return(fs,psd)\n",
    "\n",
    "def accu_sensi_speci(y_pred, y_truth):\n",
    "    y_p = np.zeros(len(y_pred))\n",
    "    y_t = np.zeros(len(y_pred))\n",
    "    for i in range(len(y_p)):\n",
    "        if y_pred[i,0] > y_pred[i,1]:\n",
    "            y_p[i] = 0\n",
    "        else:\n",
    "            y_p[i] = 1\n",
    "    for i in range(len(y_t)):\n",
    "        if y_truth[i,0] > y_truth[i,1]:\n",
    "            y_t[i] = 0\n",
    "        else:\n",
    "            y_t[i] = 1\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for i in range(len(y_t)):\n",
    "        if y_t[i]==y_p[i]==1:\n",
    "            tp+=1\n",
    "        if y_t[i]==1 and y_p[i]==0:\n",
    "            fn+=1\n",
    "        if y_t[i]==0 and y_p[i]==1:\n",
    "            fp+=1\n",
    "        if y_t[i]==y_p[i]==0:\n",
    "            tn+=1\n",
    "    sensi = tp / (tp + fn)\n",
    "    speci = tn / (tn + fp)\n",
    "    accu = np.sum(y_t==y_p) / len(y_p)\n",
    "    return(accu, sensi, speci)\n",
    "\n",
    "def harmonize_data(X,y):\n",
    "    _, class_0_idx = np.where(y==0)\n",
    "    _, class_1_idx = np.where(y==1)\n",
    "\n",
    "    class_0_idx = np.where(class_0_idx == 1)\n",
    "    class_1_idx = np.where(class_1_idx == 1)\n",
    "\n",
    "    idx_0 = random.choices(class_0_idx[0], k=int(len(class_1_idx[0])*1.5))\n",
    "    idx = np.concatenate([idx_0, class_1_idx[0]])\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "    \n",
    "    return(X,y)\n",
    "\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, classIdx, layerName):\n",
    "        self.model = model\n",
    "        self.classIdx = classIdx\n",
    "        self.layerName = layerName\n",
    "\n",
    "    def compute_heatmap(self, image, eps=1e-8):\n",
    "        gradModel = Model(\n",
    "            inputs = [self.model.inputs],\n",
    "            outputs = [self.model.get_layer(self.layerName).output, self.model.output])\n",
    "\t\t\n",
    "        with tf.GradientTape() as tape:\n",
    "            inputs = tf.cast(image, tf.float32)\n",
    "            (convOutputs, predictions) = gradModel(inputs)\n",
    "            loss = predictions[:, self.classIdx]\n",
    "        grads = tape.gradient(loss, convOutputs)\n",
    "        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
    "        castGrads = tf.cast(grads > 0, \"float32\")\n",
    "        guidedGrads = castConvOutputs * castGrads * grads\n",
    "        convOutputs = convOutputs[0]\n",
    "        guidedGrads = guidedGrads[0]\n",
    "        weights = tf.reduce_mean(guidedGrads, axis=(0,1))\n",
    "        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "        (w, h) = (image.shape[2], image.shape[1])\n",
    "        heatmap = cv2.resize(cam.numpy(), (w, h))\n",
    "        numer = heatmap - np.min(heatmap)\n",
    "        denom = (heatmap.max() - heatmap.min()) + eps\n",
    "        heatmap = numer / denom\n",
    "        heatmap = (heatmap * 255).astype(\"uint8\")\n",
    "        return(heatmap)\n",
    "\n",
    "    def correlation(self, image, truth):\n",
    "        pred = self.model.predict(image)\n",
    "        i = np.argmax(pred[0])\n",
    "        gradcam = GradCAM(self.model, i, \"Conv3\")\n",
    "        heatmap = gradcam.compute_heatmap(image)\n",
    "        title = \"class = \" + str(truth) + ', pred = ' + str(np.round(np.squeeze(pred), 2))\n",
    "        x = np.arange(900)\n",
    "        rri_weights = (heatmap[:,0] - np.mean(heatmap[:,0])) / (np.std(heatmap[:,0]) * len(heatmap[:,0]))\n",
    "        rra_weights = (heatmap[:,1] - np.mean(heatmap[:,1])) / (np.std(heatmap[:,1]) * len(heatmap[:,1]))\n",
    "        edr_weights = (heatmap[:,2] - np.mean(heatmap[:,2])) / (np.std(heatmap[:,2]) * len(heatmap[:,2]))\n",
    "\n",
    "        rri_norm = (image[0,:,0,0] - np.mean(image[0,:,0,0])) / (np.std(image[0,:,0,0]) * len(image[0,:,0,0]))\n",
    "        rra_norm = (image[0,:,1,0] - np.mean(image[0,:,1,0])) / (np.std(image[0,:,1,0]) * len(image[0,:,1,0]))\n",
    "        edr_norm = (image[0,:,2,0] - np.mean(image[0,:,2,0])) / (np.std(image[0,:,2,0]) * len(image[0,:,2,0]))\n",
    "\n",
    "        rri_correl = np.correlate(rri_norm, rri_weights)\n",
    "        rra_correl = np.correlate(rra_norm, rra_weights)\n",
    "        edr_correl = np.correlate(edr_norm, edr_weights)\n",
    "\n",
    "        correl = np.zeros((3,3))\n",
    "        signals = [rri_weights, rra_weights, edr_weights]\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                correl[i,j] = np.correlate(signals[i], signals[j])\n",
    "        correl = np.multiply(correl, 1/correl[0,0])\n",
    "        print(correl)\n",
    "        return()\n",
    "\n",
    "\n",
    "    def plot_heatmap(self, image, truth):\n",
    "        pred = self.model.predict(image)\n",
    "        i = np.argmax(pred[0])\n",
    "        gradcam = GradCAM(self.model, i, \"Conv3\")\n",
    "        heatmap = gradcam.compute_heatmap(image)\n",
    "        title = \"class = \" + str(truth) + ', pred = ' + str(np.round(np.squeeze(pred), 2))\n",
    "        x = np.arange(900)\n",
    "        rri_weights = (heatmap[:,0] - np.min(heatmap[:,0]))\n",
    "        rri_weights = rri_weights / np.max(rri_weights) * (np.max(image[:,:,0]) - np.min(image[:,:,0])) + np.max(image[:,:,0])\n",
    "        rra_weights = (heatmap[:,1] - np.min(heatmap[:,1]))\n",
    "        rra_weights = rra_weights / np.max(rra_weights) * (np.max(image[:,:,1]) - np.min(image[:,:,1])) + np.max(image[:,:,1])\n",
    "        plt.subplot(211)\n",
    "        plt.plot(x, rri_weights, x, np.squeeze(image[:,:,0]))\n",
    "        plt.legend(['Weights', 'RRi'])\n",
    "        plt.xlabel('Sample')\n",
    "        plt.xlim([20,880])\n",
    "        #plt.vlines(x=[360,540], ymin=np.min(image[:,:,0]), ymax=np.max(rri_weights), color='red')\n",
    "        plt.subplot(212)\n",
    "        plt.plot(x, rra_weights, x, np.squeeze(image[:,:,1]))\n",
    "        plt.legend(['Weights', 'RRamp'])\n",
    "        plt.xlabel('Sample')\n",
    "        plt.xlim([20,880])\n",
    "        #plt.vlines(x=[360,540], ymin=np.min(image[::,:,0]), ymax=np.max(rra_weights), color='red')\n",
    "        if image.shape[2] == 3:\n",
    "            edr_weights = (heatmap[:,2] - np.min(heatmap[:,2]))\n",
    "            edr_weights = edr_weights / np.max(edr_weights) * (np.max(image[:,:,2]) - np.min(image[:,:,2])) + np.max(image[:,:,2])\n",
    "            plt.subplot(311)\n",
    "            plt.plot(x, rri_weights, x, np.squeeze(image[:,:,0]))\n",
    "            plt.legend(['Weights', 'RRi'])\n",
    "            plt.xlabel('Sample')\n",
    "            plt.xlim([20,880])\n",
    "            #plt.vlines(x=[360,540], ymin=np.min(image[:,:,0]), ymax=np.max(rri_weights), color='red')\n",
    "            plt.subplot(312)\n",
    "            plt.plot(x, rra_weights, x, np.squeeze(image[:,:,1]))\n",
    "            plt.legend(['Weights', 'RRamp'])\n",
    "            plt.xlabel('Sample')\n",
    "            plt.xlim([20,880])\n",
    "            plt.suptitle(title)\n",
    "            plt.subplot(313)\n",
    "            plt.plot(x, edr_weights, x, np.squeeze(image[:,:,2]))\n",
    "            plt.legend(['Weights', 'EDR'])\n",
    "            plt.xlabel('Sample')\n",
    "            plt.xlim([20,880])\n",
    "            plt.suptitle(title)\n",
    "        return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CIC database\n",
    "X, sleep_stages, sleep_apnea, sleep_apnea_locs, groups = load_data_cic_new(base_dir='./', file_name='cic_database_train_apnea.pkl')\n",
    "#n_class = 4\n",
    "#X_lstm, y_lstm = convert_to_LSTM(X, y, groups, n_class)\n",
    "#y_cnn = convert_to_CNN(y, n_class)\n",
    "sleep_stages_cnn = np.array(to_categorical(sleep_stages))\n",
    "sleep_apnea_cnn = np.array(to_categorical(sleep_apnea))\n",
    "X = np.swapaxes(X, 1, 2)\n",
    "del_idx = []\n",
    "for i in range(X.shape[0]):\n",
    "        if np.isnan(np.sum(X[i,:,2])):\n",
    "                del_idx.append(i)\n",
    "        \n",
    "X_clean = np.delete(X, del_idx, 0)\n",
    "sleep_stages_cnn_clean = np.delete(sleep_stages_cnn, del_idx, 0)\n",
    "sleep_apnea_cnn_clean = np.delete(sleep_apnea_cnn, del_idx, 0)\n",
    "sleep_apnea_locs_clean = np.delete(sleep_apnea_locs, del_idx, 0)\n",
    "groups_clean = np.delete(groups, del_idx, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CIC DATABASE\n",
    "%reload_ext tensorboard\n",
    "from datetime import datetime\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 60 and \\\n",
    "            (epoch - 1) % 10 == 0:\n",
    "        lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "opti_loss = \"categorical_crossentropy\" # \"categorical_crossentropy\", weighted_categorical_crossentropy\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, sleep_apnea_cnn_clean, train_size=0.01, test_size=0.01)\n",
    "print('n_train = ', X_train.shape[0],', n_test = ', X_test.shape[0])\n",
    "\n",
    "CNN_n_layers=3\n",
    "CNN_kernel_size=[5,5,5]\n",
    "CNN_n_nodes=[32,64,128]\n",
    "pooling_size=[3,3,2]\n",
    "cnn_strides=[2,2,1]\n",
    "dil_CNN_n_layers=1\n",
    "dil_CNN_kernel_size=[3]\n",
    "dil_CNN_n_nodes=[32]\n",
    "dil_pooling_size=[3]\n",
    "dil_cnn_strides=[1]\n",
    "\n",
    "save_title = str(CNN_n_layers) + str(CNN_kernel_size) + str(cnn_strides) + str(pooling_size) + str(dil_CNN_n_layers) + str(dil_CNN_kernel_size) + str(dil_cnn_strides) + str(dil_pooling_size)\n",
    "title = str(CNN_n_layers) + ' convolutional layers, kernel size ' + str(CNN_kernel_size) + ', strides ' + str(cnn_strides) + ', pooling size ' + str(pooling_size)\n",
    "title = title + '\\n' + str(dil_CNN_n_layers) + ' dilated convolutional layers, kernel size ' + str(dil_CNN_kernel_size) + ', strides ' + str(dil_cnn_strides) + ', pooling size ' + str(dil_pooling_size)\n",
    "model = create_CNN((X_train.shape[1], X_train.shape[2], 1), normalize=False, n_class=2, \n",
    "                    CNN_n_layers=3, CNN_kernel_size=CNN_kernel_size, CNN_n_nodes=CNN_n_nodes, pooling_size=pooling_size, cnn_strides=cnn_strides,\n",
    "                    dil_CNN_n_layers=dil_CNN_n_layers, dil_CNN_kernel_size=dil_CNN_kernel_size, dil_CNN_n_nodes=dil_CNN_n_nodes, \n",
    "                    dil_pooling_size=dil_pooling_size, dil_cnn_strides=dil_cnn_strides, \n",
    "                    CNN_n_feature=64, drop_rate=0.2, weight=1e-3)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=opti_loss, metrics=['categorical_accuracy'], weighted_metrics=['categorical_accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, callbacks=[lr_scheduler], verbose=2,  validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accu, sensi, speci = accu_sensi_speci(y_pred, y_test)\n",
    "print(accu, sensi, speci)\n",
    "save_title = str(CNN_n_layers) + str(CNN_kernel_size) + str(cnn_strides) + str(pooling_size) + str(dil_CNN_n_layers) + str(dil_CNN_kernel_size) + str(dil_cnn_strides) + str(dil_pooling_size)\n",
    "title = str(CNN_n_layers) + ' convolutional layers, kernel size ' + str(CNN_kernel_size) + ', strides ' + str(cnn_strides) + ', pooling size ' + str(pooling_size)\n",
    "title = title + '\\n' + str(dil_CNN_n_layers) + ' dilated convolutional layers, kernel size ' + str(dil_CNN_kernel_size) + ', strides ' + str(dil_cnn_strides) + ', pooling size ' + str(dil_pooling_size)\n",
    "\n",
    "plt.figure(figsize=[10, 6])\n",
    "plt.plot(history.epoch, history.history['categorical_accuracy'], label='categorical_accuracy', color='blue')\n",
    "#plt.plot(history.epoch, history.history['weighted_categorical_accuracy'], label='weighted_categorical_accuracy', color='blue', linestyle='--')\n",
    "plt.plot(history.epoch, history.history['val_categorical_accuracy'], label='val_categorical_accuracy', color='orange')\n",
    "#plt.plot(history.epoch, history.history['val_weighted_categorical_accuracy'], label='val_weighted_categorical_accuracy', color='orange', linestyle='--')\n",
    "plt.title(title)\n",
    "plt.axvline(70)\n",
    "plt.axvline(80)\n",
    "plt.axvline(90)\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim([0.5,1])\n",
    "plt.xlim([0,100])\n",
    "plt.savefig(save_title + '.jpg')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "conf_matrix_CNN(X_test, y_test, model) \n",
    "title = 'accuracy ' + str(np.round(accu, 2)) + ', sensibility ' + str(np.round(sensi, 2)) + ', specificity ' + str(np.round(speci, 2))\n",
    "plt.title(title)\n",
    "plt.savefig(save_title + 'mat.jpg')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "734afe24b50bfa0c2c34d475b83700efda0d6c607ab642709701935adeb4285b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('sleep_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
